# -*- coding: utf-8 -*-
"""425 PROJECT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18cy84589Uzi4szMaKQnAy92L6HnYhmGj
"""

!pip install -U datasets fsspec

from datasets import load_dataset

dataset = load_dataset("ErfanMoosaviMonazzah/fake-news-detection-dataset-English")
# Remove the 'label' column from each split
dataset = dataset.remove_columns("label")

# Check the columns in the training set
#print(dataset["train"].column_names)

## no
# View the dataset splits
#print(dataset)

# Access the first example in the training set
#print(dataset["train"][0])

# Access train split with
texts = dataset["train"]["text"]

# If you want a subset:
subset_texts = dataset["train"][:int(len(dataset["train"]) * 0.1)]["text"]

# Pre- processing
# pre trained model bert
!pip install -U sentence-transformers
import re
# Clean the text
def clean_text(text):
    text = text.lower()
    text = re.sub(r"[^a-zA-Z0-9\s]", "", text)
    return text

texts = [clean_text(t) for t in dataset["train"]["text"]]

# Step 2: Get Sentence Embeddings
from sentence_transformers import SentenceTransformer
#model_sbert = SentenceTransformer("all-MiniLM-L6-v2")
model_sbert = SentenceTransformer("all-mpnet-base-v2")
X = model_sbert.encode(texts, convert_to_numpy=True, show_progress_bar=True)

# Normalize
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X = scaler.fit_transform(X)

#Define the Neural Network Model
# Step 3: Define Autoencoder
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class Autoencoder(nn.Module):
    def __init__(self, input_size, hidden_size, latent_size):
        super(Autoencoder, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, latent_size)
        )
        self.decoder = nn.Sequential(
            nn.Linear(latent_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, input_size),
            nn.Sigmoid()
        )

    def forward(self, x):
        z = self.encoder(x)
        x_recon = self.decoder(z)
        return x_recon, z

# Step 4: Train Autoencoder

input_size = X.shape[1]
hidden_size = 128
latent_size = 64

model = Autoencoder(input_size, hidden_size, latent_size)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

inputs = torch.tensor(X, dtype=torch.float32)
epochs = 10

for epoch in range(epochs):
    recon, _ = model(inputs)
    loss = criterion(recon, inputs)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    print(f"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}")

# Step 5: DEC Clustering Model
class DEC(nn.Module):
    def __init__(self, encoder, n_clusters, alpha=1.0):
        super(DEC, self).__init__()
        self.encoder = encoder
        self.alpha = alpha
        self.cluster_centers = nn.Parameter(torch.randn(n_clusters, latent_size))

    def forward(self, x):
        z = self.encoder(x)
        q = 1.0 / (1.0 + torch.sum((z.unsqueeze(1) - self.cluster_centers)**2, dim=2) / self.alpha)
        q = q ** ((self.alpha + 1.0) / 2.0)
        q = q / torch.sum(q, dim=1, keepdim=True)
        return q, z

def target_distribution(q):
    weight = q ** 2 / q.sum(0)
    return (weight.T / weight.sum(1)).T

def kl_divergence(p, q):
    return torch.sum(p * torch.log(p / (q + 1e-8)), dim=1).mean()

# Initialize DEC with trained encoder
dec_model = DEC(model.encoder, n_clusters=4)
dec_optimizer = torch.optim.Adam(dec_model.parameters(), lr=0.001)

max_dec_epochs = 20
update_interval = 5
tol = 1e-3
prev_q = None

inputs = inputs  # your input tensor

# Phase 2: DEC Fine-tuning loop
for epoch in range(max_dec_epochs):
    dec_model.train()
    q, z = dec_model(inputs)

    if epoch % update_interval == 0:
        p = target_distribution(q).detach()

        if prev_q is not None:
            delta_label = torch.sum(torch.abs(q - prev_q)) / q.shape[0]
            if delta_label < tol:
                print(f"DEC converged at epoch {epoch}")
                break
        prev_q = q.clone().detach()

    loss = kl_divergence(p, q)

    dec_optimizer.zero_grad()
    loss.backward()
    dec_optimizer.step()

    print(f"DEC Epoch [{epoch+1}/{max_dec_epochs}], Loss: {loss.item():.4f}")

# Latent embedding from Standard AE
model.eval()
with torch.no_grad():
    _, z_standard = model(inputs)
z_standard_np = z_standard.numpy()

# Save embeddings
np.save("standard_latent.npy", z_standard_np)

# Save DEC-refined latent representations from standard autoencoder
z_standard_dec_np = z.detach().numpy()
np.save("standard_latent_DEC.npy", z_standard_dec_np)

# Step 5: Clustering Evaluation
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

z_np = z.detach().numpy()

# KMeans
kmeans = KMeans(n_clusters=4, random_state=0)
kmeans_labels = kmeans.fit_predict(z_np)
print("KMeans Silhouette:", silhouette_score(z_np, kmeans_labels))
print("KMeans DB Index:", davies_bouldin_score(z_np, kmeans_labels))
print("KMeans Calinski-Harabasz:", calinski_harabasz_score(z_np, kmeans_labels))

# DBSCAN
dbscan = DBSCAN(eps=1.0, min_samples=5)
dbscan_labels = dbscan.fit_predict(z_np)
print("DBSCAN Silhouette:", silhouette_score(z_np, dbscan_labels))
print("DBSCAN DB Index:", davies_bouldin_score(z_np, dbscan_labels))
print("DBSCAN Calinski-Harabasz:", calinski_harabasz_score(z_np, dbscan_labels))

# Hierarchical
hierarchical = AgglomerativeClustering(n_clusters=4)
hierarchical_labels = hierarchical.fit_predict(z_np)
print("Hierarchical Silhouette:", silhouette_score(z_np, hierarchical_labels))
print("Hierarchical DB Index:", davies_bouldin_score(z_np, hierarchical_labels))
print("Hierarchical Calinski-Harabasz:", calinski_harabasz_score(z_np, hierarchical_labels))

np.save("kmeans_labels_standard.npy", kmeans_labels)
np.save("dbscan_labels_standard.npy", dbscan_labels)
np.save("hierarchical_labels_standard.npy", hierarchical_labels)

# Vizualization
# PCA for standard auotoencoder
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

pca = PCA(n_components=2)
reduced_embeddings = pca.fit_transform(z_np)

plt.figure(figsize=(8, 6))
plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=kmeans_labels, cmap='tab10')
plt.title("PCA: KMeans Clusters")
plt.colorbar()
plt.show()

plt.figure(figsize=(8, 6))
plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=dbscan_labels, cmap='tab10')
plt.title("PCA: DBSCAN Clusters")
plt.colorbar()
plt.show()

plt.figure(figsize=(8, 6))
plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=hierarchical_labels, cmap='tab10')
plt.title("PCA: Hierarchical Clusters")
plt.colorbar()
plt.show()

#tsne
from sklearn.manifold import TSNE

tsne = TSNE(n_components=2, perplexity=30, random_state=42)
tsne_embeddings = tsne.fit_transform(z_np)

plt.figure(figsize=(8, 6))
plt.scatter(tsne_embeddings[:, 0], tsne_embeddings[:, 1], c=kmeans_labels, cmap='tab10')
plt.title("t-SNE: KMeans Clusters")
plt.colorbar()
plt.show()

plt.figure(figsize=(8, 6))
plt.scatter(tsne_embeddings[:, 0], tsne_embeddings[:, 1], c=dbscan_labels, cmap='tab10')
plt.title("t-SNE: DBSCAN Clusters")
plt.colorbar()
plt.show()

plt.figure(figsize=(8, 6))
plt.scatter(tsne_embeddings[:, 0], tsne_embeddings[:, 1], c=hierarchical_labels, cmap='tab10')
plt.title("t-SNE: Hierarchical Clusters")
plt.colorbar()
plt.show()

# Meem Autoencoder Definition
class Meem_Autoencoder(nn.Module):
    def __init__(self, input_size, latent_size):
        super(Meem_Autoencoder, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_size, 512),
            nn.BatchNorm1d(512),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.2),

            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.2),

            nn.Linear(256, latent_size)
        )
        self.decoder = nn.Sequential(
            nn.Linear(latent_size, 256),
            nn.BatchNorm1d(256),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.2),

            nn.Linear(256, 512),
            nn.BatchNorm1d(512),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.2),

            nn.Linear(512, input_size),
            nn.Sigmoid()
        )

    def forward(self, x):
        z = self.encoder(x)
        x_recon = self.decoder(z)
        return x_recon, z

# Train Meem model
input_size = X.shape[1]
latent_size = 64
inputs = torch.tensor(X, dtype=torch.float32)

model = Meem_Autoencoder(input_size, latent_size)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

for epoch in range(10):
    model.train()
    recon, _ = model(inputs)
    loss = criterion(recon, inputs)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    print(f"Meem_Autoencoder Epoch [{epoch+1}/10], Loss: {loss.item():.4f}")

# Latent embedding
model.eval()
with torch.no_grad():
    _, z_Meem = model(inputs)
z_Meem_np = z_Meem.numpy()

# Save embeddings
np.save("Meem_latent.npy", z_Meem_np)

# Step: DEC Clustering for Meem Autoencoder

# Use the encoder from the trained Meem model
class DEC(nn.Module):
    def __init__(self, encoder, n_clusters, alpha=1.0):
        super(DEC, self).__init__()
        self.encoder = encoder
        self.alpha = alpha
        self.cluster_centers = nn.Parameter(torch.randn(n_clusters, latent_size))

    def forward(self, x):
        z = self.encoder(x)
        q = 1.0 / (1.0 + torch.sum((z.unsqueeze(1) - self.cluster_centers)**2, dim=2) / self.alpha)
        q = q ** ((self.alpha + 1.0) / 2.0)
        q = q / torch.sum(q, dim=1, keepdim=True)
        return q, z

def target_distribution(q):
    weight = q ** 2 / q.sum(0)
    return (weight.T / weight.sum(1)).T

def kl_divergence(p, q):
    return torch.sum(p * torch.log(p / (q + 1e-8)), dim=1).mean()

# Initialize DEC using the trained Meem_Autoencoder encoder
dec_model = DEC(model.encoder, n_clusters=4)
dec_optimizer = torch.optim.Adam(dec_model.parameters(), lr=0.001)

max_dec_epochs = 20
update_interval = 5
tol = 1e-3
prev_q = None

# Phase 2: DEC Fine-tuning loop
for epoch in range(max_dec_epochs):
    dec_model.train()
    q, z = dec_model(inputs)

    if epoch % update_interval == 0:
        p = target_distribution(q).detach()

        if prev_q is not None:
            delta_label = torch.sum(torch.abs(q - prev_q)) / q.shape[0]
            if delta_label < tol:
                print(f"DEC converged at epoch {epoch}")
                break
        prev_q = q.clone().detach()

    loss = kl_divergence(p, q)

    dec_optimizer.zero_grad()
    loss.backward()
    dec_optimizer.step()

    print(f"Meem DEC Epoch [{epoch+1}/{max_dec_epochs}], Loss: {loss.item():.4f}")

# Extract DEC-refined latent representations
z_Meem_dec_np = z.detach().numpy()

# Optionally overwrite or save separately
np.save("Meem_latent_DEC.npy", z_Meem_dec_np)

#Clustering Evaluation for Meem Autoencoder
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

# Use DEC-refined embeddings from Meem_Autoencoder
z_Meem_eval = z_Meem_dec_np

# KMeans
kmeans_Meem = KMeans(n_clusters=4, random_state=0)
kmeans_labels_Meem = kmeans_Meem.fit_predict(z_Meem_eval)
print("Meem KMeans Silhouette:", silhouette_score(z_Meem_eval, kmeans_labels_Meem))
print("Meem KMeans DB Index:", davies_bouldin_score(z_Meem_eval, kmeans_labels_Meem))
print("Meem KMeans Calinski-Harabasz:", calinski_harabasz_score(z_Meem_eval, kmeans_labels_Meem))

# DBSCAN
dbscan_Meem = DBSCAN(eps=1.0, min_samples=5)
dbscan_labels_Meem = dbscan_Meem.fit_predict(z_Meem_eval)
n_unique_labels = len(set(dbscan_labels_Meem)) - (1 if -1 in dbscan_labels_Meem else 0)

if n_unique_labels > 1:
    print("Meem DBSCAN Silhouette:", silhouette_score(z_Meem_eval, dbscan_labels_Meem))
    print("Meem DBSCAN DB Index:", davies_bouldin_score(z_Meem_eval, dbscan_labels_Meem))
    print("Meem DBSCAN Calinski-Harabasz:", calinski_harabasz_score(z_Meem_eval, dbscan_labels_Meem))
else:
    print("Meem DBSCAN: Not enough clusters to compute metrics (found 1 or fewer clusters).")
# Hierarchical
hierarchical_Meem = AgglomerativeClustering(n_clusters=4)
hierarchical_labels_Meem = hierarchical_Meem.fit_predict(z_Meem_eval)
print("Meem Hierarchical Silhouette:", silhouette_score(z_Meem_eval, hierarchical_labels_Meem))
print("Meem Hierarchical DB Index:", davies_bouldin_score(z_Meem_eval, hierarchical_labels_Meem))
print("Meem Hierarchical Calinski-Harabasz:", calinski_harabasz_score(z_Meem_eval, hierarchical_labels_Meem))

np.save("kmeans_labels_Meem.npy", kmeans_labels_Meem)
np.save("dbscan_labels_Meem.npy", dbscan_labels_Meem)
np.save("hierarchical_labels_Meem.npy", hierarchical_labels_Meem)

#PCA for Meeme auotoencoder
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

pca = PCA(n_components=2)
reduced_Meem = pca.fit_transform(z_Meem_dec_np)

plt.figure(figsize=(8, 6))
plt.scatter(reduced_Meem[:, 0], reduced_Meem[:, 1], c=kmeans_labels_Meem, cmap='tab10')
plt.title("PCA (Meem_AE + Meem_DEC): KMeans Clusters")
plt.colorbar()
plt.show()

plt.figure(figsize=(8, 6))
plt.scatter(reduced_Meem[:, 0], reduced_Meem[:, 1], c=dbscan_labels_Meem, cmap='tab10')
plt.title("PCA (Meem_AE + Meem_DEC): DBSCAN Clusters")
plt.colorbar()
plt.show()

plt.figure(figsize=(8, 6))
plt.scatter(reduced_Meem[:, 0], reduced_Meem[:, 1], c=hierarchical_labels_Meem, cmap='tab10')
plt.title("PCA (Meem_AE + Meem_DEC): Hierarchical Clusters")
plt.colorbar()
plt.show()

from sklearn.manifold import TSNE

tsne = TSNE(n_components=2, perplexity=30, random_state=42)
tsne_Meem = tsne.fit_transform(z_Meem_dec_np)

plt.figure(figsize=(8, 6))
plt.scatter(tsne_Meem[:, 0], tsne_Meem[:, 1], c=kmeans_labels_Meem, cmap='tab10')
plt.title("t-SNE (Meem_AE + Meem_DEC): KMeans Clusters")
plt.colorbar()
plt.show()

plt.figure(figsize=(8, 6))
plt.scatter(tsne_Meem[:, 0], tsne_Meem[:, 1], c=dbscan_labels_Meem, cmap='tab10')
plt.title("t-SNE (Meem_AE + Meem_DEC): DBSCAN Clusters")
plt.colorbar()
plt.show()

plt.figure(figsize=(8, 6))
plt.scatter(tsne_Meem[:, 0], tsne_Meem[:, 1], c=hierarchical_labels_Meem, cmap='tab10')
plt.title("t-SNE (Meem_AE + Meem_DEC): Hierarchical Clusters")
plt.colorbar()
plt.show()

# Comparision

import pandas as pd
import numpy as np
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

#  KMEANS
metrics_df = pd.DataFrame({
    "Metric": [
        "Silhouette Score",
        "Davies-Bouldin Index",
        "Calinski-Harabasz Index"
    ],
    "Standard AE (KMeans)": [
        silhouette_score(z_standard_dec_np, kmeans_labels),
        davies_bouldin_score(z_standard_dec_np, kmeans_labels),
        calinski_harabasz_score(z_standard_dec_np, kmeans_labels)
    ],
    "Meem AE (KMeans)": [
        silhouette_score(z_Meem_dec_np, kmeans_labels_Meem),
        davies_bouldin_score(z_Meem_dec_np, kmeans_labels_Meem),
        calinski_harabasz_score(z_Meem_dec_np, kmeans_labels_Meem)
    ]
})

print("\nClustering Evaluation Summary (KMeans)")
print(metrics_df.to_string(index=False))

#DBSCAN
def safe_clustering_metrics(X, labels):
    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
    if n_clusters < 2:
        return ["N/A", "N/A", "N/A"]
    return [
        silhouette_score(X, labels),
        davies_bouldin_score(X, labels),
        calinski_harabasz_score(X, labels)
    ]

standard_dbscan_scores = safe_clustering_metrics(z_standard_dec_np, dbscan_labels)
Meem_dbscan_scores = safe_clustering_metrics(z_Meem_dec_np, dbscan_labels_Meem)

metrics_dbscan = pd.DataFrame({
    "Metric": [
        "Silhouette Score",
        "Davies-Bouldin Index",
        "Calinski-Harabasz Index"
    ],
    "Standard AE (DBSCAN)": standard_dbscan_scores,
    "Meem AE (DBSCAN)": Meem_dbscan_scores
})

print("\nClustering Evaluation Summary (DBSCAN)")
print(metrics_dbscan.to_string(index=False))

# HIERARCHICAL
metrics_hier = pd.DataFrame({
    "Metric": [
        "Silhouette Score",
        "Davies-Bouldin Index",
        "Calinski-Harabasz Index"
    ],
    "Standard AE (Hierarchical)": [
        silhouette_score(z_standard_dec_np, hierarchical_labels),
        davies_bouldin_score(z_standard_dec_np, hierarchical_labels),
        calinski_harabasz_score(z_standard_dec_np, hierarchical_labels)
    ],
    "Meem AE (Hierarchical)": [
        silhouette_score(z_Meem_dec_np, hierarchical_labels_Meem),
        davies_bouldin_score(z_Meem_dec_np, hierarchical_labels_Meem),
        calinski_harabasz_score(z_Meem_dec_np, hierarchical_labels_Meem)
    ]
})

print("\n Clustering Evaluation Summary (Hierarchical)")
print(metrics_hier.to_string(index=False))

fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# PCA Standard
pca_std = PCA(n_components=2).fit_transform(z_standard_dec_np)
axes[0, 0].scatter(pca_std[:, 0], pca_std[:, 1], c=kmeans_labels, cmap='tab10')
axes[0, 0].set_title("PCA - Standard AE")

# PCA Meem
pca_Meem = PCA(n_components=2).fit_transform(z_Meem_dec_np)
axes[0, 1].scatter(pca_Meem[:, 0], pca_Meem[:, 1], c=kmeans_labels_Meem, cmap='tab10')
axes[0, 1].set_title("PCA - Meem AE")

# t-SNE Standard
tsne_std = TSNE(n_components=2, perplexity=30, random_state=42).fit_transform(z_standard_dec_np)
axes[1, 0].scatter(tsne_std[:, 0], tsne_std[:, 1], c=kmeans_labels, cmap='tab10')
axes[1, 0].set_title("t-SNE - Standard AE")

# t-SNE Meem
tsne_Meem = TSNE(n_components=2, perplexity=30, random_state=42).fit_transform(z_Meem_dec_np)
axes[1, 1].scatter(tsne_Meem[:, 0], tsne_Meem[:, 1], c=kmeans_labels_Meem, cmap='tab10')
axes[1, 1].set_title("t-SNE - Meem AE")

plt.tight_layout()
plt.show()

# Install graphviz if not already available
!apt-get install -y graphviz
!pip install graphviz

from graphviz import Digraph
from IPython.display import Image, display

# Define the diagram
dot = Digraph(comment='Meem Autoencoder + DEC Pipeline', format='png')

# Layout direction: top-to-bottom
dot.attr(rankdir='TB', size='10')

# Nodes (Steps)
dot.node('A', 'Load Fake News Dataset')
dot.node('B', 'Text Cleaning (Regex)')
dot.node('C', 'Sentence Embedding\n(SentenceTransformer: all-mpnet-base-v2)')
dot.node('D', 'StandardScaler Normalization')
dot.node('E', 'Meem Autoencoder Training\n(10 Epochs)')
dot.node('F', 'Latent Embedding Extraction (z_Meem)')
dot.node('G', 'DEC Clustering\n(20 Epochs)')
dot.node('H', 'DEC-refined Embeddings (z_Meem_dec)')
dot.node('I', 'Clustering Algorithms\n(KMeans, DBSCAN, Agglomerative)')
dot.node('J', 'Evaluation Metrics\n(Silhouette, DB, Calinski-Harabasz)')
dot.node('K', 'Save Cluster Labels (.npy)')
dot.node('L', 'Visualization\n(PCA & t-SNE)')

# Connections
dot.edges(['AB', 'BC', 'CD', 'DE', 'EF', 'FG', 'GH', 'HI', 'IJ', 'JK', 'KL'])

# Render diagram to file and display in Colab
dot.render('meem_pipeline_colab', format='png', cleanup=False)

# Display the image in the output cell
display(Image(filename='meem_pipeline_colab.png'))

# Install graphviz (if not already installed)
!apt-get install -y graphviz
!pip install graphviz

from graphviz import Digraph
from IPython.display import Image, display

# Create the diagram
dot = Digraph(comment='Meem Autoencoder + DEC Pipeline', format='png')

# Set layout direction
dot.attr(rankdir='TB', size='10')
dot.attr('node', shape='box')  # Set default node shape to box

# Nodes
dot.node('A', 'Load Fake News Dataset')
dot.node('B', 'Text Cleaning (Regex)')
dot.node('C', 'Sentence Embedding\n(SentenceTransformer: all-mpnet-base-v2)')
dot.node('D', 'StandardScaler Normalization')
dot.node('E', 'Meem Autoencoder Training\n(10 Epochs)')
dot.node('F', 'Latent Embedding Extraction (z_Meem)')
dot.node('G', 'DEC Clustering\n(20 Epochs)')
dot.node('H', 'DEC-refined Embeddings (z_Meem_dec)')
dot.node('I', 'Clustering Algorithms\n(KMeans, DBSCAN, Agglomerative)')
dot.node('J', 'Evaluation Metrics\n(Silhouette, DB, Calinski-Harabasz)')
dot.node('K', 'Save Cluster Labels (.npy)')
dot.node('L', 'Visualization\n(PCA & t-SNE)')

# Edges
dot.edges(['AB', 'BC', 'CD', 'DE', 'EF', 'FG', 'GH', 'HI', 'IJ', 'JK', 'KL'])

# Render and display the diagram
dot.render('meem_pipeline_colab_boxed', format='png', cleanup=False)
display(Image(filename='meem_pipeline_colab_boxed.png'))